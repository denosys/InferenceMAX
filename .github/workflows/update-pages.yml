name: nightly-update-pages
on:
  schedule:
    - cron: "0 2 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      SECRET_PAT: ${{ secrets.SECRET_PAT }}
      PUBLIC_PAGE: https://inferencemax.semianalysis.com
      ARTIFACTS_DIR: data_zips
      DOCS_DATA: docs/data
      NODE_VERSION: "20"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install system deps
        run: |
          sudo apt-get update
          sudo apt-get install -y jq unzip curl ca-certificates fonts-liberation

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Add puppeteer helper and scripts
        run: |
          mkdir -p .github/scripts
          cat > .github/scripts/fetch_page.js <<'NODEJS'
          const puppeteer = require('puppeteer');
          const fs = require('fs');
          function sleep(ms) { return new Promise(resolve => setTimeout(resolve, ms)); }
          (async () => {
            const url = process.argv[2];
            const out = process.argv[3] || '/tmp/im_public.html';
            if (!url) { console.error('Missing url'); process.exit(2); }
            console.log('Rendering URL:', url);
            const browser = await puppeteer.launch({
              args: ['--no-sandbox','--disable-setuid-sandbox','--disable-dev-shm-usage'],
              headless: true
            });
            try {
              const page = await browser.newPage();
              await page.goto(url, { waitUntil: 'networkidle2', timeout: 60000 });
              await sleep(1000);
              const html = await page.content();
              fs.writeFileSync(out, html);
              console.log('Saved rendered page to', out);
              await browser.close();
              process.exit(0);
            } catch (e) {
              console.error('Rendering failed:', e);
              try { await browser.close(); } catch {}
              process.exit(1);
            }
          })();
          NODEJS
          cat > .github/scripts/parse_runs.sh <<'SH'
          #!/usr/bin/env bash
          set -euo pipefail
          infile="$1"
          out="$2"
          mkdir -p "$(dirname "$out")" /tmp/im_runs
          tmp_out="${out}.abs"
          : > "$tmp_out"
          echo "Parsing run links from $infile"
          grep -Eo 'https?://github.com/[^/]+/[^/]+/actions/runs/[0-9]+' "$infile" >> "$tmp_out" || true
          grep -Eo '/[^/]+/[^/]+/actions/runs/[0-9]+' "$infile" | sed 's|^|https://github.com|' >> "$tmp_out" || true
          sort -u "$tmp_out" > "$out" || true
          echo "Parsed run list written to $out (size $(stat -c%s "$out" || echo 0) bytes)"
          wc -l "$out" || true
          SH
          cat > .github/scripts/download_artifacts.sh <<'SH'
          #!/usr/bin/env bash
          set -euo pipefail
          runs_file="$1"
          artifacts_dir="$2"
          docs_data="$3"
          secret_pat="$4"
          mkdir -p "$artifacts_dir" "$docs_data" /tmp/im_runs
          read -r -d '' ALLOWED <<'EOT' || true
          results_gptoss_1k1k.zip
          results_gptoss_8k1k.zip
          results_gptoss_1k8k.zip
          results_70b_1k1k.zip
          results_70b_8k1k.zip
          results_70b_1k8k.zip
          results_dsr1_1k1k.zip
          results_dsr1_8k1k.zip
          results_dsr1_1k8k.zip
          EOT
          is_allowed() {
            local name="$1"
            while IFS= read -r line; do
              [ "$line" = "$name" ] && return 0
            done <<< "$ALLOWED"
            return 1
          }
          AUTH_HDR="Authorization: token ${secret_pat}"

          fetch_all_artifacts_for_run() {
            local owner="$1" repo="$2" run_id="$3" out_json="$4"
            : > "$out_json"
            local page=1
            while :; do
              api_url="https://api.github.com/repos/${owner}/${repo}/actions/runs/${run_id}/artifacts?per_page=100&page=${page}"
              tmp="$(mktemp)"
              http_code=$(curl -sS -w "%{http_code}" -H "Accept: application/vnd.github+json" -H "$AUTH_HDR" "$api_url" -o "$tmp") || http_code="000"
              if [ "$http_code" != "200" ]; then
                echo "  Warning: API request failed for page ${page} (http ${http_code})"
                rm -f "$tmp"
                break
              fi
              jq -c '.artifacts[]' "$tmp" >> "$out_json"
              link_hdr=$(curl -sI -H "Accept: application/vnd.github+json" -H "$AUTH_HDR" "$api_url" | tr -d '\r' | sed -n 's/^Link: //Ip' || true)
              rm -f "$tmp"
              if echo "$link_hdr" | grep -q 'rel="next"'; then
                page=$((page+1))
                continue
              else
                break
              fi
            done
          }

          echo "Reading runs file: $runs_file"
          if [ ! -f "$runs_file" ]; then
            echo "Runs file not found: $runs_file"
            exit 1
          fi

          # iterate runs and download/extract allowed artifacts
          while IFS= read -r run_url || [ -n "$run_url" ]; do
            [ -z "$run_url" ] && continue
            owner=$(echo "$run_url" | awk -F'/' '{print $4}')
            repo=$(echo "$run_url" | awk -F'/' '{print $5}')
            run_id=$(echo "$run_url" | awk -F'/' '{print $NF}')
            echo "Processing run $run_id from $owner/$repo"
            tmp_art="/tmp/im_runs/artifacts_${run_id}.ndjson"
            rm -f "$tmp_art"
            fetch_all_artifacts_for_run "$owner" "$repo" "$run_id" "$tmp_art"
            if [ ! -s "$tmp_art" ]; then
              echo " No artifacts for run $run_id (artifact list empty)"
              continue
            fi
            echo " Artifacts metadata file: $tmp_art (size $(stat -c%s "$tmp_art"))"
            wc -l "$tmp_art" || true

            # iterate items (each line is a JSON artifact)
            while IFS= read -r art || [ -n "$art" ]; do
              [ -z "$art" ] && continue
              art_id=$(echo "$art" | jq -r '.id')
              art_name=$(echo "$art" | jq -r '.name')
              norm_name="$art_name"
              case "$norm_name" in *.zip) ;; *) norm_name="${norm_name}.zip" ;; esac
              if ! is_allowed "$norm_name"; then
                echo " Skipping artifact not in allowed list: $art_name"
                continue
              fi
              archive_url=$(echo "$art" | jq -r '.archive_download_url')
              safe_name="${run_id}-${art_id}-${art_name}"
              out="${artifacts_dir}/${safe_name}.zip"
              echo " Downloading artifact $art_name from run $run_id -> $out"
              http_code=$(curl -sSL -w "%{http_code}" -H "Accept: application/vnd.github+json" -H "$AUTH_HDR" -o "$out" "$archive_url" || echo "000")
              echo "  Download returned HTTP code: $http_code"
              echo "  Downloaded to $out (size: $(stat -c%s "$out" || echo 0) bytes)"
              echo "  Listing zip contents:"
              unzip -l "$out" || echo "  unzip -l failed for $out"

              if [ -s "$out" ] && unzip -t "$out" >/dev/null 2>&1; then
                echo "  Zip test OK for $out"
                tmpd=$(mktemp -d)
                echo "  Extracting to $tmpd"
                unzip -q "$out" -d "$tmpd"
                echo "  After extract, listing extracted files:"
                while IFS= read -r -d '' f; do
                ls -l -- "f"∣∣truedone<<(find"tmpd" -type f -print0) || true

                # Save extracted files with run-artid prefix to avoid overwriting
                find "$tmpd" -type f $ -name "agg_*.json" -o -name "*__agg_*.json" -o -name "*.json" -o -iname "*.png" -o -iname "*.jpg" -o -iname "*.jpeg" $ -print0 \
                  | while IFS= read -r -d '' f; do
                      echo "   Found extracted file: $f"
                      dest_base=$(basename "$f" | sed -E 's/[^A-Za-z0-9._-]+/_/g')
                      prefixed="${run_id}-${art_id}-${dest_base}"
                      cp -f "$f" "${docs_data}/${prefixed}"
                      echo "   -> extracted ${prefixed}"
                  done
                rm -rf "$tmpd"
                # copy zip for offline inspection
                cp -f "$out" "/tmp/inspected_$(basename "$out")" || true
              else
                echo "  Warning: artifact $out not a valid zip or empty"
              fi
            done < "$tmp_art"
          done < "$runs_file"

          # --- global post-process (run once after all runs processed) ---
          echo "Starting global post-process"
          docs_data="${docs_data%/}"
          older_dir="${docs_data}/older"
          mkdir -p "$older_dir"

          parse_prefixed() {
            local bn="$1"
            if [[ "$bn" =~ ^([0-9]+)-([0-9]+)-(.+)$ ]]; then
              printf '%s\t%s\t%s' "${BASH_REMATCH[1]}" "${BASH_REMATCH[2]}" "${BASH_REMATCH[3]}"
              return 0
            fi
            return 1
          }

          declare -A best_for_subtype
          declare -A best_run
          declare -A best_artid
          non_prefixed_files=()

          echo "Building stable list of files in $docs_data"
          mapfile -t allfiles < <(find "$docs_data" -maxdepth 1 -type f -printf '%P\n' | sed 's/^[[:space:]]*//; s/[[:space:]]*$//' | grep -v '^$' | sort)
          echo "Found ${#allfiles[@]} files in $docs_data (root)"

          for bn in "${allfiles[@]}"; do
            f="${docs_data}/${bn}"
            if parsed=$(parse_prefixed "$bn" 2>/dev/null); then
              IFS=$'\t' read -r run artid namepart <<< "$parsed"
              run=$(printf '%s' "$run" | tr -cd '0-9'); artid=$(printf '%s' "$artid" | tr -cd '0-9')
              [ -z "$run" ] && run=0; [ -z "$artid" ] && artid=0
              subtype=$(printf '%s' "$namepart" | sed -E 's/__agg_.*$//' | sed -E 's/\.[^.]+$//')
              prev_run=${best_run[$subtype]:--1}
              prev_art=${best_artid[$subtype]:-0}
              echo "DBG: bn='${bn}' -> run=${run} artid=${artid} subtype='${subtype}' prev_run=${prev_run} prev_art=${prev_art}"
              if (( run > prev_run )) || { (( run == prev_run )) && (( artid > prev_art )); }; then
                best_run[$subtype]="$run"
                best_artid[$subtype]="$artid"
                best_for_subtype[$subtype]="$f"
              fi
            else
              if [[ -n "$f" ]]; then
                non_prefixed_files+=("$f")
              fi
            fi
          done

          echo "Moving older prefixed files to $older_dir and keeping winners"
          for bn in "${allfiles[@]}"; do
            f="${docs_data}/${bn}"
            if parse_prefixed "$bn" >/dev/null 2>&1; then
              keep_this=0
              for s in "${!best_for_subtype[@]}"; do
                if [ "$(basename "${best_for_subtype[$s]}")" = "$bn" ]; then
                  keep_this=1
                  break
                fi
                if [ "$(readlink -f "${best_for_subtype[$s]}" 2>/dev/null || true)" = "$(readlink -f "$f" 2>/dev/null || true)" ]; then
                  keep_this=1
                  break
                fi
              done
              if [ "$keep_this" -eq 1 ]; then
                echo "Keeping prefixed file: ${bn}"
              else
                echo "Moving older prefixed file: ${bn} -> ${older_dir}/"
                mv -f "$f" "${older_dir}/" || echo " Failed to move $f"
              fi
            fi
          done

          echo "Deleting non-prefixed files in root of $docs_data"
          for f in "${non_prefixed_files[@]:-}"; do
            [ -n "$f" ] || continue
            bn=$(basename "$f")
            echo "Deleting non-prefixed file: ${bn}"
            rm -f "$f" || echo " Failed to delete $f"
          done

          echo "Copying winners to canonical basenames"
          for s in "${!best_for_subtype[@]}"; do
            winner="${best_for_subtype[$s]}"
            win_bn=$(basename "$winner")
            canonical_bn="$(printf '%s' "$win_bn" | sed -E 's/^[0-9]+-[0-9]+-//')"
            if [ -n "$canonical_bn" ]; then
              cp -f "${docs_data}/${win_bn}" "${docs_data}/${canonical_bn}"
              echo "Finalized winner for ${s}: ${canonical_bn} (from ${win_bn})"
            fi
          done

          echo "Global post-process complete. Kept files:"
          for s in "${!best_for_subtype[@]}"; do
            echo "  - $(basename "${best_for_subtype[$s]}")"
          done

          echo "Artifacts dir listing:"
          ls -la "${artifacts_dir}" || true
          echo "Docs data root listing:"
          ls -la "${docs_data}" || true
          echo "Docs data recursive listing (maxdepth 2):"
          find "${docs_data}" -maxdepth 2 -type f -printf '%P\n' | sed 's/^[[:space:]]*//; s/[[:space:]]*$//' | sort || true
          SH
          chmod +x .github/scripts/*.sh

      - name: Install node deps (puppeteer)
        run: |
          npm init -y
          npm i puppeteer --no-audit --no-fund

      - name: Render public page with Puppeteer
        env:
          PUBLIC_PAGE: ${{ env.PUBLIC_PAGE }}
        run: |
          node .github/scripts/fetch_page.js "${PUBLIC_PAGE}" /tmp/im_public.html
          ls -la /tmp/im_public.html

      - name: Parse run links from rendered page
        run: |
          .github/scripts/parse_runs.sh /tmp/im_public.html /tmp/im_runs/list.txt
          echo "Run links:"
          cat /tmp/im_runs/list.txt || true

      - name: Download allowed artifacts from runs
        env:
          SECRET_PAT: ${{ env.SECRET_PAT }}
          ARTIFACTS_DIR: ${{ env.ARTIFACTS_DIR }}
          DOCS_DATA: ${{ env.DOCS_DATA }}
        run: |
          .github/scripts/download_artifacts.sh /tmp/im_runs/list.txt "${ARTIFACTS_DIR}" "${DOCS_DATA}" "${SECRET_PAT}"

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install python deps
        run: pip install --upgrade pip && pip install pandas plotly

      - name: Generate HTML
        run: |
          set -euo pipefail
          mkdir -p docs data_zips
          python3 -u generate_html.py || true
          find docs -maxdepth 2 -type f -print || true
          ls -la docs/data || true

      - name: Commit and push generated site
        run: |
          set -euo pipefail
          git config --local user.email "actions@github.com"
          git config --local user.name "GitHub Actions"
          git remote set-url origin https://x-access-token:${{ secrets.SECRET_PAT }}@github.com/${{ github.repository }}.git
          git add docs || true
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "nightly: update pages and data"
            git push origin HEAD
          fi
