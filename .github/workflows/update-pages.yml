name: nightly-update-pages
on:
  schedule:
    - cron: "0 2 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      SECRET_PAT: ${{ secrets.SECRET_PAT }}
      REPO_OWNER: InferenceMAX
      REPO_NAME: InferenceMAX
      PUBLIC_PAGE: https://inferencemax.semianalysis.com
      ARTIFACTS_DIR: data_zips
      DOCS_DATA: docs/data

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install jq and unzip
        run: sudo apt-get update && sudo apt-get install -y jq unzip

      - name: Download selected artifacts from public page runs
        run: |
          set -euo pipefail

          mkdir -p "${ARTIFACTS_DIR}" "${DOCS_DATA}" /tmp/im_runs

          # Allowed artifact basenames (exact)
          read -r -d '' ALLOWED <<'EOT' || true
          results_gptoss_1k1k.zip
          results_gptoss_8k1k.zip
          results_gptoss_1k8k.zip
          results_70b_1k1k.zip
          results_70b_8k1k.zip
          results_70b_1k8k.zip
          results_dsr1_1k1k.zip
          results_dsr1_8k1k.zip
          results_dsr1_1k8k.zip
          EOT

          is_allowed() {
            local name="$1"
            while IFS= read -r line; do
              [ "$line" = "$name" ] && return 0
            done <<< "$ALLOWED"
            return 1
          }

          # 1) Download public page (no token needed)
          curl -sS "${PUBLIC_PAGE}" -o /tmp/im_public.html

          # 2) Extract run URLs (GitHub actions run links)
          grep -Eo 'https://github.com/[^/]+/[^/]+/actions/runs/[0-9]+' /tmp/im_public.html | sort -u > /tmp/im_runs/list.txt

          if [ ! -s /tmp/im_runs/list.txt ]; then
            echo "No run links found on public page."
            exit 0
          fi

          # 3) For each run URL, call GitHub API to list artifacts and download allowed artifacts
          while read -r run_url; do
            run_id=$(echo "$run_url" | awk -F'/' '{print $NF}')
            echo "Processing run $run_id from $run_url"

            api_url="https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}/actions/runs/${run_id}/artifacts?per_page=100"
            curl -sS -H "Accept: application/vnd.github+json" -H "Authorization: token ${GITHUB_TOKEN}" "$api_url" -o /tmp/im_runs/artifacts_${run_id}.json

            if ! jq -e '.artifacts | length > 0' /tmp/im_runs/artifacts_${run_id}.json >/dev/null 2>&1; then
              echo " No artifacts for run $run_id"
              continue
            fi

            jq -c '.artifacts[]' /tmp/im_runs/artifacts_${run_id}.json | while read -r art; do
              art_id=$(echo "$art" | jq -r '.id')
              art_name=$(echo "$art" | jq -r '.name')

              # Normalize to ensure .zip suffix for comparison
              norm_name="$art_name"
              case "$norm_name" in
                *.zip) ;;
                *) norm_name="${norm_name}.zip" ;;
              esac

              if ! is_allowed "$norm_name"; then
                echo " Skipping artifact not in allowed list: $art_name"
                continue
              fi

              archive_url=$(echo "$art" | jq -r '.archive_download_url')
              safe_name="${run_id}-${art_id}-${art_name}"
              out="${ARTIFACTS_DIR}/${safe_name}.zip"

              echo " Downloading allowed artifact: $art_name -> $out"
              http_code=$(curl -sSL -w "%{http_code}" -H "Accept: application/vnd.github+json" -H "Authorization: token ${GITHUB_TOKEN}" -o "$out" "$archive_url" || echo "000")
              if [ "$http_code" != "200" ]; then
                echo "  Warning: failed to download artifact $art_id (http $http_code)"
                rm -f "$out" || true
                continue
              fi

              if [ -s "$out" ] && unzip -t "$out" >/dev/null 2>&1; then
                tmpd=$(mktemp -d)
                unzip -q "$out" -d "$tmpd"
                find "$tmpd" -type f $ -name "agg_*.json" -o -name "*.json" -o -iname "*.png" -o -iname "*.jpg" -o -iname "*.jpeg" $ -print0 \
                  | while IFS= read -r -d '' f; do
                      dest=$(basename "$f" | sed -E 's/[^A-Za-z0-9._-]+/_/g')
                      cp -f "$f" "${DOCS_DATA}/${dest}"
                      echo "   -> extracted ${dest}"
                  done
                rm -rf "$tmpd"
              else
                echo "  Warning: artifact $out not a valid zip or empty"
              fi
            done

          done < /tmp/im_runs/list.txt

          echo "Done. Files copied to ${DOCS_DATA}:"
          ls -la "${DOCS_DATA}" || true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install deps
        run: pip install --upgrade pip && pip install pandas plotly

      - name: Generate HTML
        run: |
          set -euo pipefail
          echo "PWD: $(pwd)"
          echo "List data_zips:"
          ls -la data_zips || true
          mkdir -p docs data_zips
          python3 -u generate_html.py
          echo "After generate, files in docs:"
          find docs -maxdepth 2 -type f -print || true
          echo "Files in docs/data (if any):"
          ls -la docs/data || true

      - name: Commit and push generated site
        run: |
          set -euo pipefail
          git config --local user.email "actions@github.com"
          git config --local user.name "GitHub Actions"
          git remote set-url origin https://x-access-token:${{ secrets.SECRET_PAT }}@github.com/${{ github.repository }}.git
          git add docs || true
          git status --porcelain
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "nightly: update pages and data"
          fi
          git push origin HEAD
